# TailCalibX : Feature Generation for Long-tail Classification
by [Rahul Vigneswaran](https://rahulvigneswaran.github.io/), [Marc T. Law](http://www.cs.toronto.edu/~law/), [Vineeth N. Balasubramanian](https://lab1055.github.io/), [Makarand Tapaswi](https://makarandtapaswi.github.io/)

![TailCalibX methodology](readme_assets/method.svg "TailCalibX methodology")

# Table of contents
  - [ğŸ£ Easy Usage (Recommended way to use our method)](#-easy-usage-recommended-way-to-use-our-method)
    - [ğŸ’» Installation](#-installation)
    - [ğŸ‘¨â€ğŸ’» Example Code](#-example-code)
  - [ğŸ§ª Advanced Usage](#-advanced-usage)
    - [âœ” Things to do before you run the code from this repo](#-things-to-do-before-you-run-the-code-from-this-repo)
    - [ğŸ“€ How to use?](#-how-to-use)
    - [ğŸ“š How to create the mini-ImageNet-LT dataset?](#-how-to-create-the-mini-imagenet-lt-dataset)
    - [âš™ Arguments](#-arguments)
  - [ğŸ‹ï¸â€â™‚ï¸ Trained weights](#%EF%B8%8F%EF%B8%8F-trained-weights)
  - [ğŸª€ Results on a Toy Dataset](#-results-on-a-toy-dataset)
  - [ğŸŒ´ Directory Tree](#-directory-tree)
  - [ğŸ“ƒ Citation](#-citation)
  - [ğŸ‘ Contributing](#-contributing)
  - [â¤ About me](#-about-me)
  - [âœ¨ Extras](#-extras)
  - [ğŸ“ License](#-license)
  
## ğŸ£ Easy Usage (Recommended way to use our method)
âš  **Caution:**  TailCalibX is just TailCalib employed multiple times. Specifically, we generate a set of features once every epoch and use them to train the classifier. In order to mimic that, three things must be done at __every epoch__ in the following order:
1. Collect all the features from your dataloader.
2. Use the `tailcalib` package to make the features balanced by generating samples.
3. Train the classifier.
4. Repeat.

### ğŸ’» Installation
Use the package manager [pip](https://pip.pypa.io/en/stable/) to install __tailcalib__.

```bash
pip install tailcalib
```

### ğŸ‘¨â€ğŸ’» Example Code
 Check the instruction [here](tailcalib_pip/README.md) for a much more detailed python package information.

```python
# Import
from tailcalib import tailcalib

# Initialize
a = tailcalib(base_engine="numpy")   # Options: "numpy", "pytorch"

# Imbalanced random fake data
import numpy as np
X = np.random.rand(200,100)
y = np.random.randint(0,10, (200,))

# Balancing the data using "tailcalib"
feat, lab, gen = a.generate(X=X, y=y)

# Output comparison
print(f"Before: {np.unique(y, return_counts=True)}")
print(f"After: {np.unique(lab, return_counts=True)}")
```

## ğŸ§ª Advanced Usage

### âœ” Things to do before you run the code from this repo
- Change the `data_root` for your dataset in `main.py`.
- If you are using wandb logging ([Weights & Biases](https://docs.wandb.ai/quickstart)), make sure to change the `wandb.init` in `main.py` accordingly.

### ğŸ“€ How to use?
- For just the methods proposed in this paper :
    - For CIFAR100-LT: `run_TailCalibX_CIFAR100-LT.sh`
    - For mini-ImageNet-LT : `run_TailCalibX_mini-ImageNet-LT.sh`
- For all the results show in the paper :
    - For CIFAR100-LT: `run_all_CIFAR100-LT.sh`
    - For mini-ImageNet-LT : `run_all_mini-ImageNet-LT.sh`

### ğŸ“š How to create the mini-ImageNet-LT dataset?
Check `Notebooks/Create_mini-ImageNet-LT.ipynb` for the script that generates the mini-ImageNet-LT dataset with varying imbalance ratios and train-test-val splits.
### âš™ Arguments
- `--seed` : Select seed for fixing it. 
    - Default : `1`
- `--gpu` : Select the GPUs to be used. 
    - Default : `"0,1,2,3"`

- `--experiment`: Experiment number (Check 'libs/utils/experiment_maker.py'). 
    - Default : `0.1`
- `--dataset` : Dataset number. 
    - Choices : `0 - CIFAR100, 1 - mini-imagenet` 
    - Default : `0`
- `--imbalance` : Select Imbalance factor. 
    - Choices : `0: 1, 1: 100, 2: 50, 3: 10` 
    - Default : `1`
- `--type_of_val` : Choose which dataset split to use. 
    - Choices: `"vt": val_from_test, "vtr": val_from_train, "vit": val_is_test`
    - Default : `"vit"`

- `--cv1` to `--cv9` : Custom variable to use in experiments - purpose changes according to the experiment.
    - Default : `"1"`    

- `--train` : Run training sequence
    - Default : `False`
- `--generate` : Run generation sequence
    - Default : `False`
- `--retraining` : Run retraining sequence
    - Default : `False`
- `--resume` : Will resume from the 'latest_model_checkpoint.pth' and wandb if applicable.
    - Default : `False`

- `--save_features` : Collect feature representations.
    - Default : `False`
- `--save_features_phase` : Dataset split of representations to collect.
    - Choices : `"train", "val", "test"`
    - Default : `"train"`

- `--config` : If you have a yaml file with appropriate config, provide the path here. Will override the 'experiment_maker'.
    - Default : `None`

## ğŸ‹ï¸â€â™‚ï¸ Trained weights
| Experiment       | CIFAR100-LT (ResNet32, seed 1, Imb 100) | mini-ImageNet-LT (ResNeXt50)|
| -----------      | -----------        | -----------        |
| TailCalib        | [Git-LFS](https://downgit.github.io/#/home?url=https://github.com/rahulvigneswaran/trained_models/tree/main/TailCalibX/CIFAR100-LT/TailCalib)       | [Git-LFS](https://downgit.github.io/#/home?url=https://github.com/rahulvigneswaran/trained_models/tree/main/TailCalibX/mini-ImageNet-LT/TailCalib)       |
| TailCalibX       | [Git-LFS](https://downgit.github.io/#/home?url=https://github.com/rahulvigneswaran/trained_models/tree/main/TailCalibX/CIFAR100-LT/TailCalibX)        |[Git-LFS](https://downgit.github.io/#/home?url=https://github.com/rahulvigneswaran/trained_models/tree/main/TailCalibX/mini-ImageNet-LT/TailCalibX)        |
| CBD + TailCalibX | [Git-LFS](https://downgit.github.io/#/home?url=https://github.com/rahulvigneswaran/trained_models/tree/main/TailCalibX/CIFAR100-LT/CBD%2BTailCalibX)        |[Git-LFS](https://downgit.github.io/#/home?url=https://github.com/rahulvigneswaran/trained_models/tree/main/TailCalibX/mini-ImageNet-LT/CBD%2BTailCalibX)        |

## ğŸª€ Results on a Toy Dataset 
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Yj2qymSm3NgCBqvKn5r_cOiEFl9wGp3J?usp=sharing)

The higher the `Imb ratio`, the more imbalanced the dataset is.
`Imb ratio = maximum_sample_count / minimum_sample_count`.

Check [this notebook](https://colab.research.google.com/drive/1Yj2qymSm3NgCBqvKn5r_cOiEFl9wGp3J?usp=sharing) to play with the toy example from which the plot below was generated.
![](readme_assets/toy_example_output.svg)

## ğŸŒ´ Directory Tree
```bash
TailCalibX
â”œâ”€â”€ libs
â”‚   â”œâ”€â”€ core
â”‚   â”‚   â”œâ”€â”€ ce.py
â”‚   â”‚   â”œâ”€â”€ core_base.py
â”‚   â”‚   â”œâ”€â”€ ecbd.py
â”‚   â”‚   â”œâ”€â”€ modals.py
â”‚   â”‚   â”œâ”€â”€ TailCalib.py
â”‚   â”‚   â””â”€â”€ TailCalibX.py
â”‚   â”œâ”€â”€ data
â”‚   â”‚   â”œâ”€â”€ dataloader.py
â”‚   â”‚   â”œâ”€â”€ ImbalanceCIFAR.py
â”‚   â”‚   â””â”€â”€ mini-imagenet
â”‚   â”‚       â”œâ”€â”€ 0.01_test.txt
â”‚   â”‚       â”œâ”€â”€ 0.01_train.txt
â”‚   â”‚       â””â”€â”€ 0.01_val.txt
â”‚   â”œâ”€â”€ loss
â”‚   â”‚   â”œâ”€â”€ CosineDistill.py
â”‚   â”‚   â””â”€â”€ SoftmaxLoss.py
â”‚   â”œâ”€â”€ models
â”‚   â”‚   â”œâ”€â”€ CosineDotProductClassifier.py
â”‚   â”‚   â”œâ”€â”€ DotProductClassifier.py
â”‚   â”‚   â”œâ”€â”€ ecbd_converter.py
â”‚   â”‚   â”œâ”€â”€ ResNet32Feature.py
â”‚   â”‚   â”œâ”€â”€ ResNext50Feature.py
â”‚   â”‚   â””â”€â”€ ResNextFeature.py
â”‚   â”œâ”€â”€ samplers
â”‚   â”‚   â””â”€â”€ ClassAwareSampler.py
â”‚   â””â”€â”€ utils
â”‚       â”œâ”€â”€ Default_config.yaml
â”‚       â”œâ”€â”€ experiments_maker.py
â”‚       â”œâ”€â”€ globals.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ utils.py
â”œâ”€â”€ LICENSE
â”œâ”€â”€ main.py
â”œâ”€â”€ Notebooks
â”‚   â”œâ”€â”€ Create_mini-ImageNet-LT.ipynb
â”‚   â””â”€â”€ toy_example.ipynb
â”œâ”€â”€ readme_assets
â”‚   â”œâ”€â”€ method.svg
â”‚   â””â”€â”€ toy_example_output.svg
â”œâ”€â”€ README.md
â”œâ”€â”€ run_all_CIFAR100-LT.sh
â”œâ”€â”€ run_all_mini-ImageNet-LT.sh
â”œâ”€â”€ run_TailCalibX_CIFAR100-LT.sh
â””â”€â”€ run_TailCalibX_mini-imagenet-LT.sh
```
Ignored `tailcalib_pip` as it is for the `tailcalib` pip package.


## ğŸ“ƒ Citation
```
@inproceedings{rahul2021tailcalibX,
    title   = {{Feature Generation for Long-tail Classification}},
    author  = {Rahul Vigneswaran and Marc T. Law and Vineeth N. Balasubramanian and Makarand Tapaswi},
    booktitle = {ICVGIP},
    year = {2021}
}
```

## ğŸ‘ Contributing
Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.

## â¤ About me
[Rahul Vigneswaran](https://rahulvigneswaran.github.io/)

## âœ¨ Extras
[ğŸ  Long-tail buzz](https://rahulvigneswaran.github.io/longtail-buzz/) : If you are interested in deep learning research which involves __long-tailed / imbalanced__ dataset, take a look at [Long-tail buzz](https://rahulvigneswaran.github.io/longtail-buzz/) to learn about the recent trending papers in this field.

![](/readme_assets/long_tail-buzz_ss.png)

## ğŸ“ License
[MIT](LICENSE)
